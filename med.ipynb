{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Contrary to popular opinion, complex medical terminology is actually the easiest part for DeepScribe to pick up. The trickiest part for DeepScribe is to pick up on unique contextual statements a patient may give a physician. The more they stray from a typical conversation, the more we see the AI stumble.\n",
    "\n",
    "\\-- Akilesh Bapu\n",
    "\n",
    "Context is what makes language, language. Every act of speech is situated in a physical environment, and comes from an embodied agent with motivations, pre-existing knowledge, and four other senses-worth of information. This makes the difference between a sentence like \"Can you pass the salt?\" meaning for one to hand the salt to the speaker, and the speaker wondering if the listener can physically pass the salt.\n",
    "\n",
    "While non-verbal context is often critical for us when interpreting speech, language technologies often lack such data. Nonetheless, we can attempt to make intelligent assumptions about certain speech.\n",
    "\n",
    "Suppose DeepScribe was having difficulty knowing when a conversation strayed from the patient's condition to small talk. It would be helpful to identify these digressions for a number of reasons: 1) NLP technology can priortize computational resources on medically relevant speech, 2) engineers can spend less time anticipating edge case conversational speech, and more time improving the product on the speech that makes the most buiness impact, and 3) the business can save costs by storing non-medically relevant conversation in a low cost data lake.\n",
    "\n",
    "# Medetect\n",
    "\n",
    "**by Alex Liebscher**\n",
    "\n",
    "In this brief demo, I'll attempt to classify incoming message data as either medically relevant or not. As a proof-of-concept, this largely disregards many fine nuances of actual conversation, but conveys the general idea that syntactically, the two categories of speech vary enough to distinguish them.\n",
    "\n",
    "As an example of the benefits listed above, we can do a back-of-the-napkin calculation: suppose we have just under 50 TB of text data from conversations between physicians and their patients. Suppose this is stored in an S3 bucket. At \\\\$0.023 per GB per month, this costs the business about \\\\$1,150 per month on storage. \n",
    "\n",
    "Suppose that 30% of this data is simple greetings, small talk, and the occassional conversational divergence. This data might not be essential for the success of the NLP models underlying the product, but we still want to keep the data, just somewhere cheaper. When we (infrequently) want the data, we don't want to have too much delay, so we might scale this data back to an S3 One Zone tier. With 30% of the data eligible to move: $0.7*50000*0.023 + 0.3*50000*0.01 = 955$.\n",
    "\n",
    "Hence, **by identifying medically irrelevant data and moving it to a longer-term storage option, we can cut monthly storage expenses by 17%, saving nearly \\\\$200 per month.**\n",
    "\n",
    "### Overview of the demo\n",
    "\n",
    "I found two data sources (one of characters talking in movie transcripts, another of medical questions and answers) and built a prototype classification algorithm to identify new text as friendly or medical. Data loading and cleaning involved reading both text and XML files, embedding individual messages into a vector space using TF-IDF, and estimating a fine-tuned, cross-validated model.\n",
    "\n",
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import scandir\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm making use of the BERT base uncased tokenizer as a baseline. This tokenizer is slightly more sophisticated than a traditional tokenizer, since it breaks text into Byte-Pair Encodings (BPEs). These BPEs are common subwords pieces, somewhat analogous to syllables in a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick example of BPE encodings (notice how `subwords` was brooken into two component pieces, this places less weight in the final model on rare compound words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['short', 'words', 'don', \"'\", 't', 'split', ',', 'but', 'words', 'with', 'multiple', 'sub', '##words', 'do']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"short words don't split, but words with multiple subwords do\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to classify incoming messages as either medically relevant or not. The latter category, medically irrelevant, is going to be built using a dataset of lines from movies. In particular, this dataset is the [Cornell Movie-Dialogs Dataset](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). I felt like this would represent small talk fairly well. On closer inspection, I noticed there's a lot of fantasy talk in the dataset: stuff about crime scenes, or bars, or generally stuff you wouldn't (usually) find in a conversation with a medical professional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cornell-movie-dialogs-corpus/movie_lines.txt', 'rb') as f:\n",
    "    irl_data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_data_lines = []\n",
    "\n",
    "# to balance out the dataset a little better, we'll only grab at most 15k lines\n",
    "for line in irl_data[:15000]:\n",
    "    \n",
    "    try:\n",
    "        # the original data was not utf-8 encoded, and contained some meta data we throw out\n",
    "        text = line.decode('utf-8').split()[8:]\n",
    "        if text[0] == '+++$+++':\n",
    "            text = text[1:]\n",
    "\n",
    "        # to better match the next dataset, we're keeping only lines with more than 4 words\n",
    "        if len(text) > 4:\n",
    "            irl_data_lines.append(' '.join(text[:30]).lower())\n",
    "    except:\n",
    "        # for simplicity, an empty try-except to filter out a few outlier data formats\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick example of a datum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"exactly so, you going to bogey lowenbrau's thing on saturday?\""
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irl_data_lines[60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'm loading up a few sub-datasets of the [MedQuAD dataset](https://github.com/abachaa/MedQuAD), which consists of factual, medical question-answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['data/MedQuAD-master/2_GARD_QA/', 'data/MedQuAD-master/4_MPlus_Health_Topics_QA', 'data/MedQuAD-master/7_SeniorHealth_QA', ]\n",
    "\n",
    "files = [f\"{dr}/{file.name}\" for dr in dirs for file in scandir(dr)]\n",
    "\n",
    "med_data = []\n",
    "for file in files:\n",
    "    xml_data = open(file, 'r').read()\n",
    "    root = ET.XML(xml_data)\n",
    "    for qa_pair in root[2]:\n",
    "        try:\n",
    "            # we're keeping the question and the answer, but only the first 30 words of the answer (which can get too large otherwise)\n",
    "            med_data.append({\n",
    "                'q': qa_pair[0].text.lower(),\n",
    "                'a': ' '.join(qa_pair[1].text.lower().split()[:30])\n",
    "            })\n",
    "        except:\n",
    "            # for simplicity, an empty try-except to filter out a few outlier data formats\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick example datum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q': 'is chronic inflammatory demyelinating polyneuropathy inherited ?',\n",
       " 'a': 'is chronic inflammatory demyelinating polyneuropathy (cidp) inherited? cidp is not known to be inherited and is considered an acquired disorder. no clear genetic predisposition or other predisposing factors for cidp'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_data[60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 7.1k and 10.1k examples in the final medical QA and movie lines datasets, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7139, 10170)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(med_data),len(irl_data_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize data and build vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm computing the TF-IDF vector for each text sample. The TF portion of the algorithm weighs term frequencies higher, and the inverse document frequency portion decreases the weight of terms which show up in many documents. Together, higher weight is placed on tokens which are generally unique, and thus add more information to the model. This is the difference between seeing \"zebra\" and \"the\". Clearly, when we see \"zebra\", that means more for classifying that we're talking about a zoo than \"the\" would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer.tokenize)\n",
    "\n",
    "# we're only going to take the first 7k examples from both datasets, for an artifically balanced dataset\n",
    "N = 7000 \n",
    "med_data_lines = [qa['a'] for qa in med_data][:N]\n",
    "corpus = irl_data_lines[:N]\n",
    "corpus.extend(med_data_lines)\n",
    "\n",
    "# compute TF-IDF vectors for each example in the dataset\n",
    "data = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're next creating a training-testing split, so we can estimate the model using the training dataset (and potentially overfit), but lastly test our model on the test set (to see how well we generalize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.toarray()\n",
    "y = np.concatenate(([0]*N, [1]*N))\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14000, 11661), (11200, 11661), (2800, 11661), (11200,), (2800,))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, train_X.shape, test_X.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify, we have a pretty balanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4986607142857143, 0.5053571428571428)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.mean(), test_y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "We'll construct our model beginning with a vanilla logistic regression. Logistic regression offers us simplicity right off the bat, and is highly interpretable (which can help debug in the intial stages of model development).\n",
    "\n",
    "We'll fine-tune the parameter `C`, which defines how much regularization exists in the model (small numbers being near no regularization, large being a lot). There are a variety of ways to optimize hyperparameters, here we use randomized search over the parameter space. The most common technique is probably grid search, but it tends to be inefficient. Another popular optimization technique is Bayesian hyperparameter optimization. Under this paradigm, priors are set over each parameter space and then a function is estimated after sampling points in the space. The minimum or maximum (i.e. optimal) parameter combination is found in this latent space.\n",
    "\n",
    "Back to the topic at hand: we'll fit our model using 5-fold cross validation, to ensure we balance bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "params = {\n",
    "    \"C\": [0.001, 0.01, 0.1, 0.5, 1.0, 2.5, 10.0]\n",
    "}\n",
    "\n",
    "grid_search = RandomizedSearchCV(model, param_distributions=params, scoring=[\"accuracy\", \"f1\"], refit=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 7 is smaller than n_iter=10. Running 7 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score=nan,\n",
       "                   estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                dual=False, fit_intercept=True,\n",
       "                                                intercept_scaling=1,\n",
       "                                                l1_ratio=None, max_iter=100,\n",
       "                                                multi_class='auto', n_jobs=None,\n",
       "                                                penalty='l2', random_state=None,\n",
       "                                                solver='lbfgs', tol=0.0001,\n",
       "                                                verbose=0, warm_start=False),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'C': [0.001, 0.01, 0.1, 0.5, 1.0, 2.5,\n",
       "                                              10.0]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit='f1',\n",
       "                   return_train_score=False, scoring=['accuracy', 'f1'],\n",
       "                   verbose=0)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([2.03435502, 2.35411024, 4.22772932, 6.52013159, 4.96001663,\n",
       "        4.80387273, 8.22929516]),\n",
       " 'std_fit_time': array([0.98665002, 0.18216249, 0.95373969, 1.67167898, 0.15167101,\n",
       "        0.16987305, 2.56395942]),\n",
       " 'mean_score_time': array([0.07364011, 0.04572358, 0.04655681, 0.09730306, 0.04726491,\n",
       "        0.04469023, 0.10558405]),\n",
       " 'std_score_time': array([0.07942197, 0.00865253, 0.00405342, 0.09593816, 0.01044198,\n",
       "        0.00588322, 0.08816913]),\n",
       " 'param_C': masked_array(data=[0.001, 0.01, 0.1, 0.5, 1.0, 2.5, 10.0],\n",
       "              mask=[False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.001},\n",
       "  {'C': 0.01},\n",
       "  {'C': 0.1},\n",
       "  {'C': 0.5},\n",
       "  {'C': 1.0},\n",
       "  {'C': 2.5},\n",
       "  {'C': 10.0}],\n",
       " 'split0_test_accuracy': array([0.9625    , 0.971875  , 0.98794643, 0.99151786, 0.99330357,\n",
       "        0.99375   , 0.99419643]),\n",
       " 'split1_test_accuracy': array([0.95357143, 0.95714286, 0.97410714, 0.98526786, 0.98973214,\n",
       "        0.99285714, 0.99375   ]),\n",
       " 'split2_test_accuracy': array([0.96383929, 0.97098214, 0.98169643, 0.99196429, 0.99330357,\n",
       "        0.99642857, 0.99732143]),\n",
       " 'split3_test_accuracy': array([0.96696429, 0.96964286, 0.98392857, 0.99151786, 0.99330357,\n",
       "        0.99508929, 0.99598214]),\n",
       " 'split4_test_accuracy': array([0.95625   , 0.96428571, 0.97723214, 0.98794643, 0.99151786,\n",
       "        0.99375   , 0.99419643]),\n",
       " 'mean_test_accuracy': array([0.960625  , 0.96678571, 0.98098214, 0.98964286, 0.99223214,\n",
       "        0.994375  , 0.99508929]),\n",
       " 'std_test_accuracy': array([0.00495837, 0.00549379, 0.00487896, 0.00262445, 0.00142857,\n",
       "        0.00125   , 0.00135408]),\n",
       " 'rank_test_accuracy': array([7, 6, 5, 4, 3, 2, 1], dtype=int32),\n",
       " 'split0_test_f1': array([0.96171376, 0.97171082, 0.98793026, 0.991476  , 0.99327656,\n",
       "        0.9937276 , 0.99417302]),\n",
       " 'split1_test_f1': array([0.95255474, 0.95679568, 0.97399103, 0.9852349 , 0.98969996,\n",
       "        0.99284436, 0.99373882]),\n",
       " 'split2_test_f1': array([0.96349707, 0.97109827, 0.98175345, 0.9919571 , 0.99329459,\n",
       "        0.99642218, 0.99731664]),\n",
       " 'split3_test_f1': array([0.96654611, 0.96964286, 0.98391421, 0.99149127, 0.99327656,\n",
       "        0.99507389, 0.99597315]),\n",
       " 'split4_test_f1': array([0.95525114, 0.96409336, 0.97707865, 0.9878978 , 0.99146834,\n",
       "        0.99371069, 0.99416255]),\n",
       " 'mean_test_f1': array([0.95991257, 0.9666682 , 0.98093352, 0.98961141, 0.9922032 ,\n",
       "        0.99435575, 0.99507284]),\n",
       " 'std_test_f1': array([0.00521466, 0.00562136, 0.00493091, 0.00263071, 0.00143538,\n",
       "        0.00125545, 0.00136111]),\n",
       " 'rank_test_f1': array([7, 6, 5, 4, 3, 2, 1], dtype=int32)}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the raw results of the cross validation and hyperparameter tuning. It's a little difficult to read at first, but one of the most important fields here is `mean_test_f1`, which tells us the mean f1 score on the validation data for each fold while estimating. It's pretty consistently > 0.95 which is a good sign, but could signal that our input data sources are just too different from each other (and therefore too easily distinguished).\n",
    "\n",
    "Below I talk about some of the limitations of this model and steps to improve the model building procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "\n",
    "To get a quick glance at how well the model generalizes, I'm going to compute a confusion matrix for predictions on the test set. This is relevant because we can identify false positive and false negative rates, which are invaluable for diagnosing a model and making improvements.\n",
    "\n",
    "Ideally, predicting labels on the test set should be the *very* last step in model building. Realistically, it sometimes gets mixed into model development. If this happens, it's improvement to recognize that and mentally note the inherent overfitting that will happen on the next development iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = grid_search.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there were very few false negatives and positives, a good sign. An easy next step in this development would be to closely check out the examples and figure out what made them different and difficult to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1375,   10],\n",
       "       [   2, 1413]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reality check, we can feed in a couple examples of real data. Suppose we saw the following two statements in the DeepScribe datasets. Would the model above be able to identify the medically relevant one so we can cut costs on computation and data storage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_medical = \"yes at your age melanoma can be a serious concern, but you seem to be taking care of yourself\"\n",
    "text_conversation = \"yes at my son's age i know it can be a concern, but he gets along just well with the others\"\n",
    "\n",
    "grid_search.predict(vectorizer.transform([text_medical, text_conversation]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, the model was fit well enough to make the distinction.\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "This demo would be insufficient for production because of more reasons than we could count here. As a proof-of-concept, I'd say it gets the point across.\n",
    "\n",
    "Suppose we wanted to move forward with it though, what would some next steps be?\n",
    "\n",
    "1) The datasets aren't great for a number of reasons, and as we say, Garbage In, Garbage Out. Neither dataset perfectly represented any conversation we might hear in a doctor's office. Building better datasets would ideally be my first step.\n",
    "\n",
    "2) The data preprocessing is simple and lets a lot of unhelpful data get through. For example, the 4 word minimum I set on the movie lines data was pretty arbitrary. We could build better model inputs by rigorously cleaning this data, identifying patterns of poor/good input, and really understanding the data.\n",
    "\n",
    "3) The model was perhaps the simplest baseline we could get. While a simple, interpretable model is great a starting point, eventually we would need to something more sophisticated. With Huggingface making attention-based models incredibly easy to get setup, we could almost nearly jump to contextual word embeddings and recurrent neural networks to build a classifier, instead of relying on vector space embeddings and linear models. However, the trade-off is efficiency. Prepping the data and fitting the model may take magnitudes longer if we introduce these steps.\n",
    "\n",
    "4) We limited ourselves to using only the message at hand in classification. In reality, we could use a number of contextual features to improve our results, such as the previous message, the tone of the speech, the pace of the speech, and more.\n",
    "\n",
    "5) We should be comparing models, both in terms of accuracy (e.g. f1), efficiency (e.g. training time/example), and cost (e.g. hours needed to train and EC2 costs). Here we only used one model though.\n",
    "\n",
    "6) Visualization. We didn't visualize the data, the estimation process, the model output, or the predictions at all. This is crucial if we were to be comparing this model to any others.\n",
    "\n",
    "In essence, language is highly context dependent and conversations in doctors offices probably rely on context to a large extent. Non-verbal cues may also play a large role in the experience. With just speech and text data, we can nonetheless still make an attempt to work efficiently with what we're given. Classifying incoming data as either relevant to the medical aspect of the conversation or not could help improve data storage efficiency and save comptutaional resources by working only with the most relevant data.\n",
    "\n",
    "\\- Alex Liebscher"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
